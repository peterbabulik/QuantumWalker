{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFKfb2f+4GBKUE5R5dp+L+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterbabulik/QuantumWalker/blob/main/ResourceEstaminator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNINni-Ve876",
        "outputId": "8ecba87f-7c0e-4db6-caea-86fc0820e4a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Resource Estimation ---\n",
            "Parameters: N_SITES_1D = 20, DEPTH (for time context) = 50\n",
            "\n",
            "Note: RAM estimates focus on the primary state vector and ONE full operator matrix.\n",
            "      Actual peak RAM can be 2-3x higher due to intermediate matrices, Python overhead, etc.\n",
            "\n",
            "--- 1-Walker(s) ---\n",
            "  State Dimension (Number of basis states): 40\n",
            "  Est. RAM for State Vector: 0.0000 GB\n",
            "  Operator Matrix Dimensions: 40 x 40 (1.60e+03 elements)\n",
            "  Est. RAM for ONE Operator Matrix: 0.0000 GB\n",
            "  Est. RAM for History (probs, CA): 0.000008 GB\n",
            "  Minimum Est. Peak RAM (State + 1 Op + Hist): 0.0000 GB\n",
            "  Qualitative Time for 50 steps: Very Fast (seconds)\n",
            "------------------------------\n",
            "\n",
            "--- 2-Walker(s) ---\n",
            "  State Dimension (Number of basis states): 1,600\n",
            "  Est. RAM for State Vector: 0.0000 GB\n",
            "  Operator Matrix Dimensions: 1600 x 1600 (2.56e+06 elements)\n",
            "  Est. RAM for ONE Operator Matrix: 0.0381 GB\n",
            "  Est. RAM for History (probs, CA): 0.000016 GB\n",
            "  Minimum Est. Peak RAM (State + 1 Op + Hist): 0.0382 GB\n",
            "  Qualitative Time for 50 steps: Fast (seconds to a few minutes)\n",
            "------------------------------\n",
            "\n",
            "--- 3-Walker(s) ---\n",
            "  State Dimension (Number of basis states): 64,000\n",
            "  Est. RAM for State Vector: 0.0010 GB\n",
            "  Operator Matrix Dimensions: 64000 x 64000 (4.10e+09 elements)\n",
            "  Est. RAM for ONE Operator Matrix: 61.0352 GB\n",
            "  Est. RAM for History (probs, CA): 0.000023 GB\n",
            "  Minimum Est. Peak RAM (State + 1 Op + Hist): 61.0361 GB\n",
            "  Qualitative Time for 50 steps: Slow (tens of minutes to hours)\n",
            "------------------------------\n",
            "\n",
            "--- 4-Walker(s) ---\n",
            "  State Dimension (Number of basis states): 2,560,000\n",
            "  Est. RAM for State Vector: 0.0381 GB\n",
            "  Operator Matrix Dimensions: 2560000 x 2560000 (6.55e+12 elements)\n",
            "  Est. RAM for ONE Operator Matrix: 97656.2500 GB\n",
            "  Est. RAM for History (probs, CA): 0.000031 GB\n",
            "  Minimum Est. Peak RAM (State + 1 Op + Hist): 97656.2882 GB\n",
            "  Qualitative Time for 50 steps: Very Slow (hours to a day) - LIKELY MEMORY LIMITED\n",
            "------------------------------\n",
            "\n",
            "--- 5-Walker(s) ---\n",
            "  State Dimension (Number of basis states): 102,400,000\n",
            "  Est. RAM for State Vector: 1.5259 GB\n",
            "  Operator Matrix Dimensions: (102400000 x 102400000) - Impractical (Elements >10^15)\n",
            "  Est. RAM for ONE Operator Matrix: inf GB\n",
            "  Est. RAM for History (probs, CA): 0.000038 GB\n",
            "  Minimum Est. Peak RAM (State + 1 Op + Hist): inf GB\n",
            "  Qualitative Time for 50 steps: Impractical (days or more, likely memory bottlenecked first) - LIKELY MEMORY LIMITED\n",
            "------------------------------\n",
            "\n",
            "\n",
            "--- For your observed case (3 walkers, 15 sites, 10 depth) for comparison ---\n",
            "--- Resource Estimation ---\n",
            "Parameters: N_SITES_1D = 15, DEPTH (for time context) = 10\n",
            "\n",
            "Note: RAM estimates focus on the primary state vector and ONE full operator matrix.\n",
            "      Actual peak RAM can be 2-3x higher due to intermediate matrices, Python overhead, etc.\n",
            "\n",
            "--- 3-Walker(s) ---\n",
            "  State Dimension (Number of basis states): 27,000\n",
            "  Est. RAM for State Vector: 0.0004 GB\n",
            "  Operator Matrix Dimensions: 27000 x 27000 (7.29e+08 elements)\n",
            "  Est. RAM for ONE Operator Matrix: 10.8629 GB\n",
            "  Est. RAM for History (probs, CA): 0.000003 GB\n",
            "  Minimum Est. Peak RAM (State + 1 Op + Hist): 10.8634 GB\n",
            "  Qualitative Time for 10 steps: Moderate (minutes)\n",
            "------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def estimate_qw_ca_resources(n_sites_1d, num_walkers_list, depth_for_time_qualifier):\n",
        "    \"\"\"\n",
        "    Estimates computational resources for simulating N-walker QW with CA.\n",
        "\n",
        "    Args:\n",
        "        n_sites_1d (int): Number of sites in the 1D lattice.\n",
        "        num_walkers_list (list of int): A list of walker counts to estimate for.\n",
        "        depth_for_time_qualifier (int): The number of steps, used for qualitative time.\n",
        "    \"\"\"\n",
        "    print(f\"--- Resource Estimation ---\")\n",
        "    print(f\"Parameters: N_SITES_1D = {n_sites_1d}, DEPTH (for time context) = {depth_for_time_qualifier}\\n\")\n",
        "    print(\"Note: RAM estimates focus on the primary state vector and ONE full operator matrix.\")\n",
        "    print(\"      Actual peak RAM can be 2-3x higher due to intermediate matrices, Python overhead, etc.\\n\")\n",
        "\n",
        "    bytes_per_complex128 = 16\n",
        "    bytes_per_float64 = 8\n",
        "    bytes_per_int_ca = 1 # Approximate for CA cells\n",
        "\n",
        "    for num_w in num_walkers_list:\n",
        "        print(f\"--- {num_w}-Walker(s) ---\")\n",
        "\n",
        "        if n_sites_1d == 0:\n",
        "            print(\"  N_SITES_1D is 0, cannot calculate dimensions.\\n\")\n",
        "            continue\n",
        "        if num_w == 0:\n",
        "            print(\"  Number of walkers is 0.\\n\")\n",
        "            continue\n",
        "\n",
        "        # State Vector\n",
        "        try:\n",
        "            # Dimension of single walker state space\n",
        "            single_walker_state_dim = 2 * n_sites_1d\n",
        "            # Total state dimension\n",
        "            total_state_dim = np.power(single_walker_state_dim, num_w, dtype=object) # Use object to handle large numbers before overflow\n",
        "            if isinstance(total_state_dim, np.ndarray): # np.power might return array for scalar base\n",
        "                total_state_dim = total_state_dim.item()\n",
        "\n",
        "            if total_state_dim > 10**9 and num_w > 2 : # Heuristic for practically too large\n",
        "                 print(f\"  State Dimension ({single_walker_state_dim}^{num_w}) is extremely large: >10^9. Calculation likely impractical.\")\n",
        "                 ram_state_vector_gb = float('inf')\n",
        "                 ram_operator_gb = float('inf')\n",
        "                 operator_elements = float('inf')\n",
        "                 operator_dims_str = f\"({total_state_dim} x {total_state_dim}) - Impractical\"\n",
        "\n",
        "            else:\n",
        "                total_state_dim = int(total_state_dim) # Cast back if not too large\n",
        "                ram_state_vector_bytes = total_state_dim * bytes_per_complex128\n",
        "                ram_state_vector_gb = ram_state_vector_bytes / (1024**3)\n",
        "\n",
        "                # Operator Matrix (e.g., U_step)\n",
        "                operator_elements = np.power(total_state_dim, 2, dtype=object)\n",
        "                if isinstance(operator_elements, np.ndarray):\n",
        "                    operator_elements = operator_elements.item()\n",
        "\n",
        "                if operator_elements > 10**15 and num_w > 1: # Heuristic\n",
        "                    ram_operator_gb = float('inf')\n",
        "                    operator_dims_str = f\"({total_state_dim} x {total_state_dim}) - Impractical (Elements >10^15)\"\n",
        "                else:\n",
        "                    operator_elements = int(operator_elements)\n",
        "                    ram_operator_bytes = operator_elements * bytes_per_complex128\n",
        "                    ram_operator_gb = ram_operator_bytes / (1024**3)\n",
        "                    operator_dims_str = f\"{total_state_dim} x {total_state_dim} ({operator_elements:.2e} elements)\"\n",
        "\n",
        "\n",
        "        except OverflowError:\n",
        "            print(f\"  OverflowError: State dimension ({ (2 * n_sites_1d) }^{num_w}) is too large to represent.\")\n",
        "            ram_state_vector_gb = float('inf')\n",
        "            ram_operator_gb = float('inf')\n",
        "            total_state_dim = float('inf')\n",
        "            operator_elements = float('inf')\n",
        "            operator_dims_str = \"Overflow\"\n",
        "\n",
        "\n",
        "        print(f\"  State Dimension (Number of basis states): {total_state_dim:,.0f}\")\n",
        "        print(f\"  Est. RAM for State Vector: {ram_state_vector_gb:.4f} GB\")\n",
        "        print(f\"  Operator Matrix Dimensions: {operator_dims_str}\")\n",
        "        print(f\"  Est. RAM for ONE Operator Matrix: {ram_operator_gb:.4f} GB\")\n",
        "\n",
        "        # History storage (less dominant but good to include for completeness)\n",
        "        if total_state_dim != float('inf'):\n",
        "            ram_prob_hist_bytes = depth_for_time_qualifier * n_sites_1d * num_w * bytes_per_float64\n",
        "            ram_ca_hist_bytes = depth_for_time_qualifier * n_sites_1d * bytes_per_int_ca\n",
        "            ram_history_gb = (ram_prob_hist_bytes + ram_ca_hist_bytes) / (1024**3)\n",
        "            print(f\"  Est. RAM for History (probs, CA): {ram_history_gb:.6f} GB\")\n",
        "\n",
        "            total_estimated_min_ram_gb = ram_state_vector_gb + ram_operator_gb + ram_history_gb\n",
        "            print(f\"  Minimum Est. Peak RAM (State + 1 Op + Hist): {total_estimated_min_ram_gb:.4f} GB\")\n",
        "\n",
        "\n",
        "        # Qualitative Time Complexity\n",
        "        # Based on StateDim^2 operations per step for matrix-vector product\n",
        "        # This is a very rough guide.\n",
        "        time_qualifier = \"N/A\"\n",
        "        if operator_elements != float('inf'):\n",
        "            ops_per_step_approx = operator_elements # from U_step @ psi (StateDim^2)\n",
        "            total_ops_approx = ops_per_step_approx * depth_for_time_qualifier\n",
        "\n",
        "            if total_ops_approx < 10**7:\n",
        "                time_qualifier = \"Very Fast (seconds)\"\n",
        "            elif total_ops_approx < 10**9:\n",
        "                time_qualifier = \"Fast (seconds to a few minutes)\"\n",
        "            elif total_ops_approx < 10**11: # Your 3-walker, 15-site, 10-depth was ~30s. (27000^2 * 10 = 7.29e9)\n",
        "                time_qualifier = \"Moderate (minutes)\"\n",
        "            elif total_ops_approx < 10**13:\n",
        "                time_qualifier = \"Slow (tens of minutes to hours)\"\n",
        "            elif total_ops_approx < 10**15:\n",
        "                time_qualifier = \"Very Slow (hours to a day)\"\n",
        "            else:\n",
        "                time_qualifier = \"Impractical (days or more, likely memory bottlenecked first)\"\n",
        "\n",
        "        if ram_operator_gb > 128: # General RAM limit for many machines\n",
        "            time_qualifier += \" - LIKELY MEMORY LIMITED\"\n",
        "        elif ram_operator_gb == float('inf'):\n",
        "            time_qualifier = \"IMPOSSIBLE DUE TO MEMORY (Operator)\"\n",
        "\n",
        "\n",
        "        print(f\"  Qualitative Time for {depth_for_time_qualifier} steps: {time_qualifier}\")\n",
        "        print(\"-\" * 30 + \"\\n\")\n",
        "\n",
        "# --- Run the calculator with your specified parameters ---\n",
        "N_SITES_PARAM = 20\n",
        "DEPTH_PARAM = 50\n",
        "walkers_to_test = [1, 2, 3, 4, 5]\n",
        "\n",
        "estimate_qw_ca_resources(N_SITES_PARAM, walkers_to_test, DEPTH_PARAM)\n",
        "\n",
        "print(\"\\n--- For your observed case (3 walkers, 15 sites, 10 depth) for comparison ---\")\n",
        "estimate_qw_ca_resources(n_sites_1d=15, num_walkers_list=[3], depth_for_time_qualifier=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output from the resource calculator is excellent and very clearly demonstrates the \"curse of dimensionality\" inherent in simulating multi-particle quantum systems directly by representing the full state vector and operators.\n",
        "\n",
        "Let's break down the implications and what this tells us about the computational hardness:\n",
        "\n",
        "**Key Takeaways from the Estimation:**\n",
        "\n",
        "1.  **Exponential Scaling is Brutal:**\n",
        "    *   **State Vector Size:** `(2 * N_SITES_1D)^W`. Each additional walker multiplies the state space dimension by `(2 * N_SITES_1D)`.\n",
        "    *   **Operator Matrix Size:** `((2 * N_SITES_1D)^W)^2`. The number of elements in the operator matrix (and thus its RAM footprint) grows as the square of the state vector size. This means it scales as `(2 * N_SITES_1D)^(2W)`. This is an incredibly rapid growth.\n",
        "\n",
        "2.  **RAM is the First Major Bottleneck:**\n",
        "    *   For `N_SITES_1D = 20`:\n",
        "        *   1-Walker: Negligible RAM.\n",
        "        *   2-Walkers: ~38 MB for the operator. Very feasible.\n",
        "        *   3-Walkers: ~61 GB for the operator. This is already at the limit or beyond for most high-end consumer PCs and standard Colab instances. You'd need a machine with substantial RAM.\n",
        "        *   4-Walkers: ~97 *Terabytes* (97656 GB) for the operator. This is squarely in the domain of supercomputers or specialized high-memory compute clusters.\n",
        "        *   5-Walkers: The numbers become so large (\"inf GB\") that they exceed practical representation and calculation limits for RAM even before considering computation time. The state vector itself is already 1.5 GB, but the operator is astronomically larger.\n",
        "\n",
        "3.  **Computational Time Follows RAM:**\n",
        "    *   While RAM hits a wall first, the number of operations per step (roughly `StateDim^2` for `U_step @ psi`) also explodes.\n",
        "    *   Even if you had infinite RAM for 4 or 5 walkers, the sheer number of calculations would make the simulation take an impractical amount of time. For 4 walkers, the qualitative estimate is \"Very Slow (hours to a day) - LIKELY MEMORY LIMITED,\" and for 5 walkers, \"Impractical (days or more, likely memory bottlenecked first).\"\n",
        "\n",
        "4.  **Your Observation (3 walkers, 15 sites, 10 depth -> ~11.9 GB RAM):**\n",
        "    *   The calculator estimated ~10.86 GB for *one* operator matrix in this scenario.\n",
        "    *   Your actual usage of 11.9 GB is very consistent with this, with the extra ~1 GB easily accounted for by:\n",
        "        *   The state vector itself.\n",
        "        *   The individual coin operator matrices (`U_Coin_A`, `U_Coin_B`, `U_Coin_C`) before they are multiplied together to form the full `U_step`. If these are `StateDim x StateDim`, you'd have several of them in memory.\n",
        "        *   Python interpreter overhead and NumPy's internal memory usage for temporary arrays during calculations.\n",
        "    *   This validates that the operator matrix size is indeed the primary RAM consumer.\n",
        "\n",
        "5.  **\"Classical Hardness\":**\n",
        "    *   This simulation method (direct state vector evolution with explicit full operators) becomes classically hard very quickly as the number of walkers (`W`) increases.\n",
        "    *   \"Hard\" here means it requires resources (RAM and/or time) that grow exponentially with the system size (number of walkers being a key component of \"size\" in this context).\n",
        "    *   For problems with `N_SITES_1D = 20`, the practical limit for this direct simulation approach on typical high-end classical hardware seems to be around **2 to 3 walkers**. Beyond that, you hit severe memory limitations.\n",
        "\n",
        "**Implications for Simulating Larger Systems:**\n",
        "\n",
        "To simulate systems with more walkers or significantly more sites using this type of coupled QW-CA model, one would need to abandon the direct construction of the full state vector and operators. Alternative approaches include:\n",
        "\n",
        "1.  **Sparse Matrix Techniques:** If the `U_step` operator is very sparse (many zero elements), specialized sparse matrix libraries can save memory and speed up computations. However, the site-dependent coin operations mean `U_step` might not be trivially sparse in a way that's easy to exploit without careful construction.\n",
        "2.  **\"On-the-Fly\" Operator Application:** If possible, compute the effect of `U_step` on `psi` without ever forming `U_step` explicitly. This is feasible if the operator has a simple structure (e.g., a sum of local terms or a product of simpler operators that can be applied sequentially). Your current code *does* apply coin and shift operators sequentially for each walker, but the *construction* of each walker's global coin operator (`U_Coin_A`, etc.) in the `build_Xwalker_full_step_operator` function still iterates over the full joint state space to build that matrix.\n",
        "3.  **Tensor Network Methods:** For certain classes of quantum systems (especially 1D or those with limited entanglement), methods like Matrix Product States (MPS) or Projected Entangled Pair States (PEPS) can represent the state vector much more efficiently if the entanglement in the system doesn't grow too rapidly. Applying operators also takes a different form. These are advanced techniques.\n",
        "4.  **Quantum Computing:** Of course, for a sufficiently large number of \"walkers\" (qubits), a quantum computer would naturally handle the exponential state space. This is one of the motivations for building quantum computers.\n",
        "5.  **Approximation Methods:** Mean-field theories, or other approximations that decouple walkers or simplify interactions, could be used, but they would lose some of the exact quantum correlations.\n",
        "\n",
        "This resource estimation clearly shows why your 3-walker simulation with 15 sites was already pushing the limits of a standard Colab environment and highlights the challenge of simulating even moderately sized interacting quantum systems on classical computers."
      ],
      "metadata": {
        "id": "C-lbF2IZiIQe"
      }
    }
  ]
}